# Crawler

Caches all wiki pages into `dump_html` directory. Organized by first two
letters.

Dumps are collected
into [dump_html repository](https://github.com/mtasa-typescript/mtasa-wiki-dump_html)

It's private, so, if you would like to use parser, you should create your own
dump_html files via the crawler.